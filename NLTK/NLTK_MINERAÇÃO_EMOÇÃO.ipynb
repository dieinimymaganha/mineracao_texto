{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = [('eu sou admirada por muitos','alegria'),\n",
    "        ('me sinto completamente amado','alegria'),\n",
    "        ('amar e maravilhoso','alegria'),\n",
    "        ('estou me sentindo muito animado novamente','alegria'),\n",
    "        ('eu estou muito bem hoje','alegria'),\n",
    "        ('que belo dia para dirigir um carro novo','alegria'),\n",
    "        ('o dia está muito bonito','alegria'),\n",
    "        ('estou contente com o resultado do teste que fiz no dia de ontem','alegria'),\n",
    "        ('o amor e lindo','alegria'),\n",
    "        ('nossa amizade e amor vai durar para sempre', 'alegria'),\n",
    "        ('estou amedrontado', 'medo'),\n",
    "        ('ele esta me ameacando a dias', 'medo'),\n",
    "        ('isso me deixa apavorada', 'medo'),\n",
    "        ('este lugar e apavorante', 'medo'),\n",
    "        ('se perdermos outro jogo seremos eliminados e isso me deixa com pavor', 'medo'),\n",
    "        ('tome cuidado com o lobisomem', 'medo'),\n",
    "        ('se eles descobrirem estamos encrencados', 'medo'),\n",
    "        ('estou tremendo de medo', 'medo'),\n",
    "        ('eu tenho muito medo dele', 'medo'),\n",
    "        ('estou com medo do resultado dos meus testes', 'medo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = ['a', 'agora', 'algum', 'alguma', 'aquele', 'aqueles', 'de', 'deu', 'do', 'e', 'estou', 'esta', 'esta',\n",
    "#              'ir', 'meu', 'muito', 'mesmo', 'no', 'nossa', 'o', 'outro', 'para', 'que', 'sem', 'talvez', 'tem', 'tendo',\n",
    "#              'tenha', 'teve', 'tive', 'todo', 'um', 'uma', 'umas', 'uns', 'vou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsnltk = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'a',\n",
       " 'o',\n",
       " 'que',\n",
       " 'e',\n",
       " 'é',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'com',\n",
       " 'não',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " 'à',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'nos',\n",
       " 'já',\n",
       " 'eu',\n",
       " 'também',\n",
       " 'só',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'até',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'você',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " 'às',\n",
       " 'minha',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'qual',\n",
       " 'nós',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'estávamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estivéramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estivéssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'há',\n",
       " 'havemos',\n",
       " 'hão',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houvéramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houvéssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houverá',\n",
       " 'houveremos',\n",
       " 'houverão',\n",
       " 'houveria',\n",
       " 'houveríamos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 'são',\n",
       " 'era',\n",
       " 'éramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'fôramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'fôssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'serão',\n",
       " 'seria',\n",
       " 'seríamos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tém',\n",
       " 'tinha',\n",
       " 'tínhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tivéramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tivéssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'terá',\n",
       " 'teremos',\n",
       " 'terão',\n",
       " 'teria',\n",
       " 'teríamos',\n",
       " 'teriam']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordsnltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removestopword(texto):\n",
    "    frases = []\n",
    "    for (palavras, emocao) in texto:\n",
    "        semstop = [p for p in palavras.split() if p not in stopwordsnltk]\n",
    "        frases.append((semstop, emocao))\n",
    "    return frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['admirada', 'muitos'], 'alegria'), (['sinto', 'completamente', 'amado'], 'alegria'), (['amar', 'maravilhoso'], 'alegria'), (['sentindo', 'animado', 'novamente'], 'alegria'), (['bem', 'hoje'], 'alegria'), (['belo', 'dia', 'dirigir', 'carro', 'novo'], 'alegria'), (['dia', 'bonito'], 'alegria'), (['contente', 'resultado', 'teste', 'fiz', 'dia', 'ontem'], 'alegria'), (['amor', 'lindo'], 'alegria'), (['amizade', 'amor', 'vai', 'durar', 'sempre'], 'alegria'), (['amedrontado'], 'medo'), (['ameacando', 'dias'], 'medo'), (['deixa', 'apavorada'], 'medo'), (['lugar', 'apavorante'], 'medo'), (['perdermos', 'outro', 'jogo', 'eliminados', 'deixa', 'pavor'], 'medo'), (['tome', 'cuidado', 'lobisomem'], 'medo'), (['descobrirem', 'encrencados'], 'medo'), (['tremendo', 'medo'], 'medo'), (['medo'], 'medo'), (['medo', 'resultado', 'testes'], 'medo')]\n"
     ]
    }
   ],
   "source": [
    "print(removestopword(base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicastemmer(texto):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    frasesstemming = []\n",
    "    for (palavras, emocao) in texto:\n",
    "        comstemming = [str(stemmer.stem(p)) for p in palavras.split() if p not in stopwordsnltk]\n",
    "        frasesstemming.append((comstemming, emocao))\n",
    "    return frasesstemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frasescomstemming = aplicastemmer(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['admir', 'muit'], 'alegria'),\n",
       " (['sint', 'complet', 'am'], 'alegria'),\n",
       " (['am', 'maravilh'], 'alegria'),\n",
       " (['sent', 'anim', 'nov'], 'alegria'),\n",
       " (['bem', 'hoj'], 'alegria'),\n",
       " (['bel', 'dia', 'dirig', 'carr', 'nov'], 'alegria'),\n",
       " (['dia', 'bonit'], 'alegria'),\n",
       " (['cont', 'result', 'test', 'fiz', 'dia', 'ont'], 'alegria'),\n",
       " (['am', 'lind'], 'alegria'),\n",
       " (['amizad', 'am', 'vai', 'dur', 'sempr'], 'alegria'),\n",
       " (['amedront'], 'medo'),\n",
       " (['ameac', 'dia'], 'medo'),\n",
       " (['deix', 'apavor'], 'medo'),\n",
       " (['lug', 'apavor'], 'medo'),\n",
       " (['perd', 'outr', 'jog', 'elimin', 'deix', 'pav'], 'medo'),\n",
       " (['tom', 'cuid', 'lobisom'], 'medo'),\n",
       " (['descobr', 'encrenc'], 'medo'),\n",
       " (['trem', 'med'], 'medo'),\n",
       " (['med'], 'medo'),\n",
       " (['med', 'result', 'test'], 'medo')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frasescomstemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscapalavras(frase):\n",
    "    todaspalavras = []\n",
    "    for (palavras, emocao) in frase:\n",
    "        todaspalavras.extend(palavras)\n",
    "    return todaspalavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras = buscapalavras(frasescomstemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admir',\n",
       " 'muit',\n",
       " 'sint',\n",
       " 'complet',\n",
       " 'am',\n",
       " 'am',\n",
       " 'maravilh',\n",
       " 'sent',\n",
       " 'anim',\n",
       " 'nov',\n",
       " 'bem',\n",
       " 'hoj',\n",
       " 'bel',\n",
       " 'dia',\n",
       " 'dirig',\n",
       " 'carr',\n",
       " 'nov',\n",
       " 'dia',\n",
       " 'bonit',\n",
       " 'cont',\n",
       " 'result',\n",
       " 'test',\n",
       " 'fiz',\n",
       " 'dia',\n",
       " 'ont',\n",
       " 'am',\n",
       " 'lind',\n",
       " 'amizad',\n",
       " 'am',\n",
       " 'vai',\n",
       " 'dur',\n",
       " 'sempr',\n",
       " 'amedront',\n",
       " 'ameac',\n",
       " 'dia',\n",
       " 'deix',\n",
       " 'apavor',\n",
       " 'lug',\n",
       " 'apavor',\n",
       " 'perd',\n",
       " 'outr',\n",
       " 'jog',\n",
       " 'elimin',\n",
       " 'deix',\n",
       " 'pav',\n",
       " 'tom',\n",
       " 'cuid',\n",
       " 'lobisom',\n",
       " 'descobr',\n",
       " 'encrenc',\n",
       " 'trem',\n",
       " 'med',\n",
       " 'med',\n",
       " 'med',\n",
       " 'result',\n",
       " 'test']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscafrequencia(palavras):\n",
    "    palavras = nltk.FreqDist(palavras)\n",
    "    return palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia = buscafrequencia(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('am', 4),\n",
       " ('dia', 4),\n",
       " ('med', 3),\n",
       " ('nov', 2),\n",
       " ('result', 2),\n",
       " ('test', 2),\n",
       " ('deix', 2),\n",
       " ('apavor', 2),\n",
       " ('admir', 1),\n",
       " ('muit', 1),\n",
       " ('sint', 1),\n",
       " ('complet', 1),\n",
       " ('maravilh', 1),\n",
       " ('sent', 1),\n",
       " ('anim', 1),\n",
       " ('bem', 1),\n",
       " ('hoj', 1),\n",
       " ('bel', 1),\n",
       " ('dirig', 1),\n",
       " ('carr', 1),\n",
       " ('bonit', 1),\n",
       " ('cont', 1),\n",
       " ('fiz', 1),\n",
       " ('ont', 1),\n",
       " ('lind', 1),\n",
       " ('amizad', 1),\n",
       " ('vai', 1),\n",
       " ('dur', 1),\n",
       " ('sempr', 1),\n",
       " ('amedront', 1),\n",
       " ('ameac', 1),\n",
       " ('lug', 1),\n",
       " ('perd', 1),\n",
       " ('outr', 1),\n",
       " ('jog', 1),\n",
       " ('elimin', 1),\n",
       " ('pav', 1),\n",
       " ('tom', 1),\n",
       " ('cuid', 1),\n",
       " ('lobisom', 1),\n",
       " ('descobr', 1),\n",
       " ('encrenc', 1),\n",
       " ('trem', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscapalavrasunicas(frequencia):\n",
    "    freq = frequencia.keys()\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['admir', 'muit', 'sint', 'complet', 'am', 'maravilh', 'sent', 'anim', 'nov', 'bem', 'hoj', 'bel', 'dia', 'dirig', 'carr', 'bonit', 'cont', 'result', 'test', 'fiz', 'ont', 'lind', 'amizad', 'vai', 'dur', 'sempr', 'amedront', 'ameac', 'deix', 'apavor', 'lug', 'perd', 'outr', 'jog', 'elimin', 'pav', 'tom', 'cuid', 'lobisom', 'descobr', 'encrenc', 'trem', 'med'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavrasunicas = buscapalavrasunicas(frequencia)\n",
    "palavrasunicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extratorpalavras(documento):\n",
    "    doc = set(documento)\n",
    "    caracteristicas = {}\n",
    "    for palavras in palavrasunicas:\n",
    "        caracteristicas['%s' % palavras] = (palavras in doc)\n",
    "    return caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'admir': False,\n",
       " 'muit': False,\n",
       " 'sint': False,\n",
       " 'complet': False,\n",
       " 'am': True,\n",
       " 'maravilh': False,\n",
       " 'sent': False,\n",
       " 'anim': False,\n",
       " 'nov': True,\n",
       " 'bem': False,\n",
       " 'hoj': False,\n",
       " 'bel': False,\n",
       " 'dia': True,\n",
       " 'dirig': False,\n",
       " 'carr': False,\n",
       " 'bonit': False,\n",
       " 'cont': False,\n",
       " 'result': False,\n",
       " 'test': False,\n",
       " 'fiz': False,\n",
       " 'ont': False,\n",
       " 'lind': False,\n",
       " 'amizad': False,\n",
       " 'vai': False,\n",
       " 'dur': False,\n",
       " 'sempr': False,\n",
       " 'amedront': False,\n",
       " 'ameac': False,\n",
       " 'deix': False,\n",
       " 'apavor': False,\n",
       " 'lug': False,\n",
       " 'perd': False,\n",
       " 'outr': False,\n",
       " 'jog': False,\n",
       " 'elimin': False,\n",
       " 'pav': False,\n",
       " 'tom': False,\n",
       " 'cuid': False,\n",
       " 'lobisom': False,\n",
       " 'descobr': False,\n",
       " 'encrenc': False,\n",
       " 'trem': False,\n",
       " 'med': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caracteristicasfrase = extratorpalavras(['am', 'nov', 'dia'])\n",
    "caracteristicasfrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "basecompleta = nltk.classify.apply_features(extratorpalavras, frasescomstemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'admir': True, 'muit': True, 'sint': False, 'complet': False, 'am': False, 'maravilh': False, 'sent': False, 'anim': False, 'nov': False, 'bem': False, 'hoj': False, 'bel': False, 'dia': False, 'dirig': False, 'carr': False, 'bonit': False, 'cont': False, 'result': False, 'test': False, 'fiz': False, 'ont': False, 'lind': False, 'amizad': False, 'vai': False, 'dur': False, 'sempr': False, 'amedront': False, 'ameac': False, 'deix': False, 'apavor': False, 'lug': False, 'perd': False, 'outr': False, 'jog': False, 'elimin': False, 'pav': False, 'tom': False, 'cuid': False, 'lobisom': False, 'descobr': False, 'encrenc': False, 'trem': False, 'med': False}, 'alegria'), ({'admir': False, 'muit': False, 'sint': True, 'complet': True, 'am': True, 'maravilh': False, 'sent': False, 'anim': False, 'nov': False, 'bem': False, 'hoj': False, 'bel': False, 'dia': False, 'dirig': False, 'carr': False, 'bonit': False, 'cont': False, 'result': False, 'test': False, 'fiz': False, 'ont': False, 'lind': False, 'amizad': False, 'vai': False, 'dur': False, 'sempr': False, 'amedront': False, 'ameac': False, 'deix': False, 'apavor': False, 'lug': False, 'perd': False, 'outr': False, 'jog': False, 'elimin': False, 'pav': False, 'tom': False, 'cuid': False, 'lobisom': False, 'descobr': False, 'encrenc': False, 'trem': False, 'med': False}, 'alegria'), ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basecompleta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     dia = True           alegri : medo   =      2.3 : 1.0\n",
      "                      am = False            medo : alegri =      1.6 : 1.0\n",
      "                     med = False          alegri : medo   =      1.4 : 1.0\n",
      "                     dia = False            medo : alegri =      1.3 : 1.0\n",
      "                     nov = False            medo : alegri =      1.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Constrou a tabela de probabilidade\n",
    "classificador = nltk.NaiveBayesClassifier.train(basecompleta)\n",
    "classificador.labels()\n",
    "classificador.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe: alegria\n",
      "alegria, 0.9316498567531292\n",
      "medo, 0.06835014324687072\n"
     ]
    }
   ],
   "source": [
    "teste = 'Hoje é um belo dia'\n",
    "testestemming = []\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "for (palavras) in teste.split():\n",
    "    comstem = [p for p in palavras.split()]\n",
    "    testestemming.append(str(stemmer.stem(comstem[0])))\n",
    "                                                  \n",
    "testestemming\n",
    "novo = extratorpalavras(testestemming)\n",
    "novo\n",
    "# Verficiando a classificação da nova frased\n",
    "print(f'Classe: {classificador.classify(novo)}')\n",
    "#Verificando a probabilidade\n",
    "\n",
    "distribuicao = classificador.prob_classify(novo)\n",
    "for classe in distribuicao.samples():\n",
    "    print(f'{classe}, {distribuicao.prob(classe)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medo'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria, 0.04104610212207739\n",
      "medo, 0.9589538978779223\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
